# CreditScoring
do some classification for credit scoring based on sickit-learn.

As public data sets for credit scoring are limited, I choose the UCI reposiory - German stalog data & Australian data.
希望可以借此项目将机器学习中的各种分类器的算法、优缺点、改进算法等都理清楚。机器学习（这里先不谈深度学习）问题的常规流程是数据采集-》数据预处理-》特征提取-》分类/聚类-》应用，无论是文本、视频、音频、图像，无论是要实现什么样的应用，文本搜索、语言模型、视频检索、机器翻译、图像识别，最终都会转化成一个一个的矩阵、向量的计算，要么分类要么聚类。然而不同的是数据类型不同，特征的裁定以及抽取方式就不同，数据量级也不同，所以这么多的研究方向虽然需要的研究基础都相似，但是要在各自领域达到顶尖就需要不同的方式。数据采集阶段，文本需要爬虫，图像、音视频也许还要人工获取，这个就是非常重要的dataset建设，其中监督学习需要的标定是最困难的，ImageNet就是在amazon上雇人标定，用众包的方式猜得到了这上千万张图和label，更多的情况就是自己在实验室辛苦地标，但是这一过程虽然靠的是体力，但是一劳永逸。数据预处理就是数字化、标准化采集得到的原始数据，文本也许就是字典学习，图片视频音频也许就是将信号数字化矩阵化，便于下一步的学习，有时还需要剔除噪声，降维等。特征提取是一个研究领域，意味着要对数据有一个representation，深度学习就是把后面的部分都省略，直接在这一步把事情解决==！但是传统的机器学习，不同数据类型会有各自不同的特征，文本的tfidf，图像的hog,lbp,pixel,sift等，这一步还需要考验人对数据的理解。分类/聚类就是学习的过程，有监督学习无监督学习这里不详述，总之机器学习靠的就是这些了。
因此，无论我将从事哪一个领域的研究，都离不开分类器，分类器再怎么发展，有些基础也是怎么也不会过时。这个项目就是我自己个人对分类器的一些理解，希望做一个记录，对今后的科研和学习有点帮助。
对这些算法的学习，主要来自于李航老师的《统计学习方法》，《PR & ML》， 《Pattern classification》.

(1)KNN

k近邻法是一种基本分类与回归方法。k值的选择、距离度量和分类决策规则是k近邻法则的三个基本要素。距离度量（Lp距离，Minkowski距离等）。

k值的大小对结果会产生重大影响：较小的k值优点是“学习”的近似误差会减小，只有与输入实例相近的训练实例才会对结果起作用，缺点是“学习”的估计误差会增大，预测结果对近邻的十里店非常敏感，受邻近噪声影响较大，整体模型变得复杂，容易过拟合；较大的k值优点是可以减少“学习”的估计误差，缺点是近似误差会增大，与输入实例较远的（不相似的）训练实例也会对预测起作用。

(2)Decision Tree

决策树是一种基本的分类与回归方法。主要优点是模型具有可读性，分类速度快。有ID3/C5.0/CART。决策树的生成只考虑局部最优，相对地，决策树的剪枝则考虑全局最优。特征选择的准则是信息增益或信息增益比。

ID3(信息增益)-》C5.0(信息增益比)-》CART(决策树是二叉树)
CART回归树的生成用最小二乘，分类树的生成用Gini指数。
(3)SVM
(4)LR
(5)NB

朴素贝叶斯方法对条件概率分布做了条件独立性的假设，这是一个较强的假设。这一假设使朴素贝叶斯法变得简单，但是优势会牺牲一定的分类准确率。

(6)ANN
(7)boosting
(8)bagging
(9)stacking
